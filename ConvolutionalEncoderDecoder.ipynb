{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Convolutional Encoder Decoder Net\n",
    "\n",
    "Usage :\n",
    "1. Download CamVid dataset ()\n",
    "2. Run createDB once (Set following condition to 1)\n",
    "# Create DB (run once)\n",
    "if (0):\n",
    "\n",
    "3. Reset condition to 0 and run training\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from TrainingPlot import *\n",
    "from PIL import Image\n",
    "import cPickle as pkl\n",
    "import time\n",
    "width = 128 # 320\n",
    "height = 128 # 224\n",
    "classes = 22\n",
    "kernelSize = 7\n",
    "featureSize = 64\n",
    "resumeTraining = True\n",
    "\n",
    "weights = {\n",
    "    'ce1': tf.get_variable(\"ce1\",shape= [kernelSize,kernelSize, 3, featureSize]),\n",
    "    'ce2': tf.get_variable(\"ce2\",shape= [kernelSize,kernelSize, featureSize, featureSize]),\n",
    "    'ce3': tf.get_variable(\"ce3\",shape= [kernelSize,kernelSize, featureSize, featureSize]),\n",
    "    'ce4': tf.get_variable(\"ce4\",shape= [kernelSize,kernelSize, featureSize, featureSize]),\n",
    "    'cd4': tf.get_variable(\"cd4\",shape= [kernelSize,kernelSize, featureSize, featureSize]),\n",
    "    'cd3': tf.get_variable(\"cd3\",shape= [kernelSize,kernelSize, featureSize, featureSize]),\n",
    "    'cd2': tf.get_variable(\"cd2\",shape= [kernelSize,kernelSize, featureSize, featureSize]),\n",
    "    'cd1': tf.get_variable(\"cd1\",shape= [kernelSize,kernelSize, featureSize, featureSize]),\n",
    "    'dense_inner_prod': tf.get_variable(\"dense_inner_prod\",shape= [1, 1, featureSize,classes])\n",
    "}\n",
    "\n",
    "def CreateDB(categoryName):\n",
    "    pathLoad1 = categoryName\n",
    "    pathLoad2 = categoryName + 'annot'\n",
    "    curPath = os.path.dirname(os.path.abspath(__file__))\n",
    "    print curPath\n",
    "    fileList1 = glob.glob(curPath + '/' + pathLoad1 + '/*.png')\n",
    "\n",
    "    #fileList2 = glob.glob(pathLoad2 + '/*.png')\n",
    "    #plt.ion()\n",
    "    trainFile = open(categoryName + '.txt','wt')\n",
    "    count = 0\n",
    "    occupancyList = []\n",
    "    for filename in fileList1:\n",
    "        img1 = Image.open(filename)\n",
    "        #filename2 = filename.replace('_IPMImg', '_IPMLabel')\n",
    "        filename2 = curPath + '/' + pathLoad2 + '/' + os.path.basename(filename)\n",
    "        img2 = Image.open(filename2)\n",
    "\n",
    "        print >> trainFile, filename\n",
    "        print >> trainFile, filename2\n",
    "\n",
    "        #cropimg.save(pathSave + '/' + filename)\n",
    "        #plt.title(pathCity)\n",
    "        #plt.imshow(cropimg)\n",
    "        #plt.show()\n",
    "        count += 1\n",
    "\n",
    "    print ('%d data list created' % count)\n",
    "    trainFile.close()\n",
    "    return occupancyList\n",
    "\n",
    "# input : [m x h x w x c]\n",
    "def Unpooling(inputOrg, size, mask=None):\n",
    "    # m, c, h, w order\n",
    "    # print 'start unpooling'\n",
    "    # size = tf.shape(inputOrg)\n",
    "    m = size[0]\n",
    "    h = size[1]\n",
    "    w = size[2]\n",
    "    c = size[3]\n",
    "    input = tf.transpose(inputOrg, [0, 3, 1, 2])\n",
    "    # print input.get_shape()\n",
    "    x = tf.reshape(input, [-1, 1])\n",
    "    k = np.float32(np.array([1.0, 1.0]).reshape([1,-1]))\n",
    "    # k = tf.Variable([1.0, 1.0],name=\"weights\")\n",
    "    # k = tf.reshape(k,[1,-1])\n",
    "    # k = np.array(k).reshape([1, -1])\n",
    "    output = tf.matmul(x, k)\n",
    "\n",
    "    output = tf.reshape(output,[-1, c, h, w * 2])\n",
    "    # m, c, w, h\n",
    "    xx = tf.transpose(output, [0, 1, 3, 2])\n",
    "    xx = tf.reshape(xx,[-1, 1])\n",
    "    # print xx.shape\n",
    "\n",
    "    output = tf.matmul(xx, k)\n",
    "    # m, c, w, h\n",
    "    output = tf.reshape(output, [-1, c, w * 2, h * 2])\n",
    "    output = tf.transpose(output, [0, 3, 2, 1])\n",
    "    # print mask\n",
    "    outshape = tf.pack([m, h * 2, w * 2, c])\n",
    "\n",
    "    if mask != None:\n",
    "        dense_mask = tf.sparse_to_dense(mask, outshape, output, 0)\n",
    "        # print dense_mask\n",
    "        # print 'output',output\n",
    "        # print 'mask',mask\n",
    "        # print dense_mask\n",
    "            # output = tf.mul(output, mask)\n",
    "\n",
    "        return output, dense_mask\n",
    "    else:\n",
    "        return output\n",
    "\n",
    "# max pool + stride 2 transpose conv\n",
    "def Model(X, W):\n",
    "\n",
    "    # Encoder\n",
    "    encoder1 = tf.nn.conv2d(X, W['ce1'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "    encoder1 = tf.nn.batch_normalization(encoder1,0.001,1.0,0,1,0.0001)\n",
    "    encoder1 = tf.nn.relu(encoder1)\n",
    "    encoder1 = tf.nn.max_pool(encoder1, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "    # encoder1 = tf.nn.dropout(encoder1, 0.5)\n",
    "\n",
    "    encoder2 = tf.nn.conv2d(encoder1, W['ce2'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "    encoder2 = tf.nn.batch_normalization(encoder2, 0.001, 1.0, 0, 1, 0.0001)\n",
    "    encoder2 = tf.nn.relu(encoder2)\n",
    "    encoder2 = tf.nn.max_pool(encoder2, ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1], padding='SAME')\n",
    "    # encoder2 = tf.nn.dropout(encoder2, 0.5)\n",
    "\n",
    "    encoder3 = tf.nn.conv2d(encoder2, W['ce3'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "    encoder3 = tf.nn.batch_normalization(encoder3, 0.001, 1.0, 0, 1, 0.0001)\n",
    "    encoder3 = tf.nn.relu(encoder3)\n",
    "    encoder3 = tf.nn.max_pool(encoder3, ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1], padding='SAME')\n",
    "    # encoder3 = tf.nn.dropout(encoder3, 0.5)\n",
    "\n",
    "    encoder4 = tf.nn.conv2d(encoder3, W['ce4'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "    encoder4 = tf.nn.batch_normalization(encoder4, 0.001, 1.0, 0, 1, 0.0001)\n",
    "    encoder4 = tf.nn.relu(encoder4)\n",
    "    encoder4 = tf.nn.max_pool(encoder4, ksize=[1, 2, 2, 1],\n",
    "                              strides=[1, 2, 2, 1], padding='SAME')\n",
    "    # encoder4 = tf.nn.dropout(encoder4, 0.5)\n",
    "\n",
    "    # Decoder\n",
    "    decoder4 = Unpooling(encoder4, [tf.shape(X)[0], height / 16, width / 16, featureSize])\n",
    "    decoder4 = tf.nn.conv2d(decoder4, W['cd4'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "    decoder4 = tf.nn.batch_normalization(decoder4, 0.001, 1.0, 0, 1, 0.0001)\n",
    "    decoder4 = tf.nn.relu(decoder4)\n",
    "    # decoder4 = tf.nn.dropout(decoder4, 0.5)\n",
    "\n",
    "    decoder3 = Unpooling(encoder3, [tf.shape(X)[0], height/8, width/8, featureSize])\n",
    "    decoder3 = tf.nn.conv2d(decoder3, W['cd3'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "    decoder3 = tf.nn.batch_normalization(decoder3, 0.001, 1.0, 0, 1, 0.0001)\n",
    "    decoder3 = tf.nn.relu(decoder3)\n",
    "    # decoder3 = tf.nn.dropout(decoder3, 0.5)\n",
    "\n",
    "    decoder2 = Unpooling(decoder3, [tf.shape(X)[0], height/4, width/4, featureSize])\n",
    "    decoder2 = tf.nn.conv2d(decoder2, W['cd2'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "    decoder2 = tf.nn.batch_normalization(decoder2, 0.001, 1.0, 0, 1, 0.0001)\n",
    "    decoder2 = tf.nn.relu(decoder2)\n",
    "    # decoder2 = tf.nn.dropout(decoder2, 0.5)\n",
    "\n",
    "    decoder1 = Unpooling(decoder2, [tf.shape(X)[0], height / 2, width / 2, featureSize])\n",
    "    decoder1 = tf.nn.conv2d(decoder1, W['cd1'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "    decoder1 = tf.nn.batch_normalization(decoder1, 0.001, 1.0, 0, 1.0, 0.0001)\n",
    "    decoder1 = tf.nn.relu(decoder1)\n",
    "    # decoder1 = tf.nn.dropout(decoder1, 0.5)\n",
    "\n",
    "    output = tf.nn.conv2d(decoder1, W['dense_inner_prod'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "    # return output, mask1, mask2, mask3\n",
    "    return output\n",
    "\n",
    "\n",
    "def DenseToOneHot(labels_dense, num_classes):\n",
    "    \"\"\"Convert class labels from scalars to one-hot vectors.\"\"\"\n",
    "    num_labels = labels_dense.shape[0]\n",
    "    index_offset = np.arange(num_labels) * num_classes\n",
    "    labels_one_hot = np.zeros((num_labels, num_classes))\n",
    "    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "    return labels_one_hot\n",
    "\n",
    "\n",
    "def LoadTrainingData(filename,sampleCount=None):\n",
    "    class DataSets(object):\n",
    "        pass\n",
    "\n",
    "    datalistFile = open(filename, \"rt\")\n",
    "    fileList = datalistFile.readlines()\n",
    "    # print len(fileList)\n",
    "    data = None\n",
    "    label = None\n",
    "    if sampleCount == None:\n",
    "        sampleCount = len(fileList)\n",
    "\n",
    "    for i in range(0,sampleCount,2):\n",
    "    # for i in range(0,50,2):\n",
    "        file = fileList[i].replace('\\n','')\n",
    "        # print ('%d / %d' % (i, len(fileList)))\n",
    "        img = Image.open(file)\n",
    "        img = img.resize((width, height))\n",
    "        rgb = np.array(img).reshape(1,height,width,3)\n",
    "\n",
    "        # pixels = np.concatenate((np.array(rgb[0]).flatten(),np.array(rgb[1]).flatten(),np.array(rgb[2]).flatten()),axis=0)\n",
    "        # pixels = pixels.reshape(pixels.shape[0], 1)\n",
    "        if i == 0:\n",
    "            data = rgb\n",
    "        else:\n",
    "            data = np.concatenate((data, rgb),axis=0)\n",
    "\n",
    "        # file = fileList[i * 2 + 1].replace('\\n', '')\n",
    "        # label = Image.open(file)\n",
    "\n",
    "        file = fileList[i+1].replace('\\n', '')\n",
    "        # print i,file\n",
    "        img = Image.open(file)\n",
    "        img = img.resize((width, height), Image.NEAREST)\n",
    "        labelImage = np.array(img).reshape(1, height, width,1)\n",
    "\n",
    "        if i == 0:\n",
    "            label = labelImage\n",
    "        else:\n",
    "            # print data.shape\n",
    "            label = np.concatenate((label, labelImage), axis=0)\n",
    "    labelOneHot = np.zeros((label.shape[0],label.shape[1], label.shape[2], classes))\n",
    "    for row in range(height):\n",
    "        for col in range(width):\n",
    "            single = label[:, row, col, 0]\n",
    "            # print single.shape\n",
    "            # exit(0)\n",
    "            # print index\n",
    "            oneHot = DenseToOneHot(single, classes)\n",
    "            labelOneHot[:, row, col, :] = oneHot\n",
    "    # for i in range(22):\n",
    "    #     plt.subplot(1,2,1)\n",
    "    #     plt.imshow(data[0,:,:,:].reshape(height,width,3))\n",
    "    #     plt.subplot(1, 2, 2)\n",
    "    #     plt.imshow(labelOneHot[0,:,:,i].reshape(height,width))\n",
    "    #     plt.show()\n",
    "    return [data.astype(np.float32)/255, label, labelOneHot.astype(np.float32)]\n",
    "\n",
    "def ShowDebuggingPlot():\n",
    "    global batchData, batchLabel\n",
    "    index = np.random.randint(trainData.shape[0])\n",
    "    batchData = trainData[index:index+1]\n",
    "    batchLabel = trainLabelOneHot[index:index+1]\n",
    "    predMaxOut = sess.run(predMax, feed_dict={x: batchData, y: batchLabel})\n",
    "    yMaxOut = sess.run(yMax, feed_dict={x: batchData, y: batchLabel})\n",
    "    # for i in range(22):\n",
    "    # show predicted image\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.title('Input')\n",
    "    img = trainData[index, :, :, :].reshape(height, width, 3)\n",
    "    plt.imshow(img * 255)\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.title('Ground truth')\n",
    "    img = yMaxOut[0, :, :].reshape(height, width)\n",
    "    plt.imshow(img)\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.title('Prediction')\n",
    "    plt.imshow(predMaxOut[0, :, :].reshape(height, width))\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.title('Error')\n",
    "    plt.imshow(img - predMaxOut[0, :, :].reshape(height, width))\n",
    "    plt.pause(0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start data loading\n",
      "Finished in 0 sec\n",
      "Strart training..\n",
      "Restoring from checkpoint ./progress-60\n",
      "#Iter 0 / 10000 : Train 0.205112, Test 1.225920, TrainAcc 0.931206, TestAcc 0.688305 (1 iter 2.43 sec, total 0.04 min, 67422.50 hour remained)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gnoses/anaconda2/envs/tensorflow8/lib/python2.7/site-packages/matplotlib/backend_bases.py:2437: MatplotlibDeprecationWarning: Using default event loop until function specific to this GUI is implemented\n",
      "  warnings.warn(str, mplDeprecation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on image #0\n",
      "#Iter 1 / 10000 : Train 0.201907, Test 1.346415, TrainAcc 0.932854, TestAcc 0.683777 (1 iter 3.01 sec, total 0.09 min, 15.09 hour remained)\n",
      "#Iter 2 / 10000 : Train 0.199136, Test 1.230863, TrainAcc 0.934105, TestAcc 0.693567 (1 iter 1.72 sec, total 0.12 min, 9.94 hour remained)\n",
      "#Iter 3 / 10000 : Train 0.196883, Test 1.347608, TrainAcc 0.934181, TestAcc 0.682703 (1 iter 1.75 sec, total 0.15 min, 8.24 hour remained)\n",
      "#Iter 4 / 10000 : Train 0.194872, Test 1.275385, TrainAcc 0.935463, TestAcc 0.692871 (1 iter 1.67 sec, total 0.18 min, 7.34 hour remained)\n",
      "#Iter 5 / 10000 : Train 0.193664, Test 1.315252, TrainAcc 0.935997, TestAcc 0.682703 (1 iter 1.79 sec, total 0.21 min, 6.86 hour remained)\n",
      "#Iter 6 / 10000 : Train 0.192238, Test 1.358321, TrainAcc 0.935692, TestAcc 0.685864 (1 iter 1.67 sec, total 0.23 min, 6.49 hour remained)\n",
      "#Iter 7 / 10000 : Train 0.191360, Test 1.255938, TrainAcc 0.936600, TestAcc 0.695276 (1 iter 1.69 sec, total 0.26 min, 6.24 hour remained)\n",
      "#Iter 8 / 10000 : Train 0.189854, Test 1.370922, TrainAcc 0.936356, TestAcc 0.678674 (1 iter 1.68 sec, total 0.29 min, 6.04 hour remained)\n",
      "#Iter 9 / 10000 : Train 0.188250, Test 1.377847, TrainAcc 0.937836, TestAcc 0.683581 (1 iter 1.74 sec, total 0.32 min, 5.91 hour remained)\n",
      "#Iter 10 / 10000 : Train 0.188445, Test 1.277009, TrainAcc 0.936920, TestAcc 0.698596 (1 iter 1.76 sec, total 0.35 min, 5.80 hour remained)\n",
      "training on image #10\n",
      "#Iter 11 / 10000 : Train 0.188450, Test 1.293694, TrainAcc 0.936752, TestAcc 0.687952 (1 iter 2.06 sec, total 0.38 min, 5.79 hour remained)\n",
      "#Iter 12 / 10000 : Train 0.192620, Test 1.504516, TrainAcc 0.933510, TestAcc 0.667920 (1 iter 1.81 sec, total 0.41 min, 5.73 hour remained)\n",
      "#Iter 13 / 10000 : Train 0.190056, Test 1.333503, TrainAcc 0.935555, TestAcc 0.689038 (1 iter 1.77 sec, total 0.44 min, 5.67 hour remained)\n",
      "#Iter 14 / 10000 : Train 0.203632, Test 1.166128, TrainAcc 0.930573, TestAcc 0.705749 (1 iter 1.82 sec, total 0.47 min, 5.62 hour remained)\n",
      "#Iter 15 / 10000 : Train 0.209822, Test 1.390800, TrainAcc 0.926529, TestAcc 0.670496 (1 iter 1.86 sec, total 0.50 min, 5.59 hour remained)\n",
      "#Iter 16 / 10000 : Train 0.202673, Test 1.281158, TrainAcc 0.930153, TestAcc 0.688599 (1 iter 1.77 sec, total 0.53 min, 5.55 hour remained)\n",
      "#Iter 17 / 10000 : Train 0.207716, Test 1.275005, TrainAcc 0.928215, TestAcc 0.692688 (1 iter 1.74 sec, total 0.56 min, 5.50 hour remained)\n",
      "#Iter 18 / 10000 : Train 0.218147, Test 1.061875, TrainAcc 0.924095, TestAcc 0.709631 (1 iter 1.85 sec, total 0.59 min, 5.48 hour remained)\n",
      "#Iter 19 / 10000 : Train 0.210728, Test 1.567325, TrainAcc 0.926613, TestAcc 0.669067 (1 iter 1.85 sec, total 0.62 min, 5.46 hour remained)\n",
      "#Iter 20 / 10000 : Train 0.205259, Test 1.112026, TrainAcc 0.928680, TestAcc 0.706079 (1 iter 1.76 sec, total 0.65 min, 5.43 hour remained)\n",
      "training on image #20\n",
      "#Iter 21 / 10000 : Train 0.205433, Test 1.303339, TrainAcc 0.929207, TestAcc 0.685681 (1 iter 2.15 sec, total 0.69 min, 5.46 hour remained)\n",
      "#Iter 22 / 10000 : Train 0.203915, Test 1.206945, TrainAcc 0.930214, TestAcc 0.678992 (1 iter 1.75 sec, total 0.72 min, 5.43 hour remained)\n",
      "#Iter 23 / 10000 : Train 0.205540, Test 1.287999, TrainAcc 0.929520, TestAcc 0.693030 (1 iter 1.73 sec, total 0.75 min, 5.40 hour remained)\n",
      "#Iter 24 / 10000 : Train 0.206335, Test 1.224908, TrainAcc 0.929527, TestAcc 0.680981 (1 iter 1.71 sec, total 0.78 min, 5.37 hour remained)\n",
      "#Iter 25 / 10000 : Train 0.201859, Test 1.243828, TrainAcc 0.930084, TestAcc 0.689831 (1 iter 1.69 sec, total 0.80 min, 5.35 hour remained)\n",
      "#Iter 26 / 10000 : Train 0.197074, Test 1.272500, TrainAcc 0.933022, TestAcc 0.682898 (1 iter 1.77 sec, total 0.83 min, 5.33 hour remained)\n",
      "#Iter 27 / 10000 : Train 0.194558, Test 1.168065, TrainAcc 0.933533, TestAcc 0.696521 (1 iter 1.77 sec, total 0.86 min, 5.31 hour remained)\n",
      "#Iter 28 / 10000 : Train 0.190546, Test 1.417611, TrainAcc 0.933815, TestAcc 0.677002 (1 iter 1.84 sec, total 0.89 min, 5.30 hour remained)\n",
      "#Iter 29 / 10000 : Train 0.188829, Test 1.155543, TrainAcc 0.935699, TestAcc 0.689526 (1 iter 1.74 sec, total 0.92 min, 5.29 hour remained)\n",
      "#Iter 30 / 10000 : Train 0.187211, Test 1.447345, TrainAcc 0.935524, TestAcc 0.683545 (1 iter 1.69 sec, total 0.95 min, 5.26 hour remained)\n",
      "training on image #30\n",
      "#Iter 31 / 10000 : Train 0.184542, Test 1.196599, TrainAcc 0.935837, TestAcc 0.683325 (1 iter 2.26 sec, total 0.99 min, 5.30 hour remained)\n",
      "#Iter 32 / 10000 : Train 0.183548, Test 1.402901, TrainAcc 0.936401, TestAcc 0.686243 (1 iter 1.75 sec, total 1.02 min, 5.28 hour remained)\n",
      "#Iter 33 / 10000 : Train 0.181221, Test 1.230546, TrainAcc 0.937607, TestAcc 0.689123 (1 iter 1.71 sec, total 1.05 min, 5.27 hour remained)\n",
      "#Iter 34 / 10000 : Train 0.181912, Test 1.326249, TrainAcc 0.937675, TestAcc 0.683923 (1 iter 1.64 sec, total 1.07 min, 5.24 hour remained)\n",
      "#Iter 35 / 10000 : Train 0.178642, Test 1.333600, TrainAcc 0.938568, TestAcc 0.687109 (1 iter 1.83 sec, total 1.10 min, 5.24 hour remained)\n",
      "#Iter 36 / 10000 : Train 0.178001, Test 1.263733, TrainAcc 0.938911, TestAcc 0.686194 (1 iter 1.72 sec, total 1.13 min, 5.22 hour remained)\n"
     ]
    }
   ],
   "source": [
    "# main body\n",
    "\n",
    "# Create DB (run once)\n",
    "if (0):\n",
    "    CreateDB('CamVid/train')\n",
    "    CreateDB('CamVid/val')\n",
    "    exit(0)\n",
    "\n",
    "startTime = time.time()\n",
    "print('Start data loading')\n",
    "sampleCount = 20\n",
    "trainData, trainLabel, trainLabelOneHot = LoadTrainingData('./CamVid/train.txt',sampleCount)\n",
    "valData, valLabel, valLabelOneHot = LoadTrainingData('./CamVid/val.txt',sampleCount/2)\n",
    "\n",
    "print('Finished in %d sec' % (time.time() - startTime))\n",
    "\n",
    "# Define functions\n",
    "x = tf.placeholder(tf.float32, [None, height,width,3])\n",
    "y = tf.placeholder(tf.float32, [None, height,width,classes])\n",
    "\n",
    "# input = tf.reshape(x, shape=[-1, 224, 320, 3])\n",
    "# output : m x height x width x classes\n",
    "\n",
    "# pred, mask1, mask2, mask3 = Model(x, weights)\n",
    "pred = Model(x, weights)\n",
    "\n",
    "linearizePred = tf.reshape(pred,shape=[-1,classes])\n",
    "linearizeY = tf.reshape(y,shape=[-1,classes])\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(linearizePred, linearizeY))\n",
    "\n",
    "\n",
    "# accuracy\n",
    "# print yNumber, tf.argmax(pred, 3)\n",
    "predMax = tf.argmax(pred, 3)\n",
    "yMax = tf.argmax(y, 3)\n",
    "correct_pred = tf.equal(tf.argmax(y,3), tf.argmax(pred, 3)) # Count correct predictions\n",
    "\n",
    "\n",
    "acc_op = tf.reduce_mean(tf.cast(correct_pred, \"float\")) # Cast boolean to float to average\n",
    "\n",
    "learning_rate = 0.0001\n",
    "\n",
    "optm = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Fit all training data\n",
    "batch_size = 2\n",
    "n_epochs = 10000\n",
    "\n",
    "print(\"Strart training..\")\n",
    "\n",
    "trainingPlot = TrainingPlot()\n",
    "trainingPlot.SetConfig(batch_size, 500, n_epochs)\n",
    "\n",
    "# you need to initialize all variables\n",
    "with tf.Session() as sess:\n",
    "    # you need to initialize all variables\n",
    "    tf.initialize_all_variables().run()\n",
    "\n",
    "    # create a log writer. run 'tensorboard --logdir=./logs/nn_logs'\n",
    "    # if (resumeTraining):\n",
    "    saver = tf.train.Saver()\n",
    "    checkpoint = tf.train.latest_checkpoint(\".\")\n",
    "    if resumeTraining == False:\n",
    "        print \"Start from scratch\"\n",
    "    elif  checkpoint:\n",
    "        print \"Restoring from checkpoint\", checkpoint\n",
    "        saver.restore(sess, checkpoint)\n",
    "    else:\n",
    "        print \"Couldn't find checkpoint to restore from. Starting over.\"\n",
    "\n",
    "\n",
    "    for epoch_i in range(n_epochs):\n",
    "        trainLoss = []\n",
    "        trainAcc = []\n",
    "        for start, end in zip(range(0, len(trainData), batch_size), range(batch_size, len(trainData), batch_size)):\n",
    "\n",
    "            batchData = trainData[start:end]\n",
    "            batchLabel = trainLabelOneHot[start:end]\n",
    "\n",
    "            sess.run(optm, feed_dict={x: batchData, y: batchLabel})\n",
    "            trainLoss.append(sess.run(cost, feed_dict={x: batchData, y: batchLabel}))\n",
    "            trainAcc.append(sess.run(acc_op, feed_dict={x: batchData, y: batchLabel}))\n",
    "\n",
    "        trainLoss = np.mean(trainLoss)\n",
    "        trainAcc = np.mean(trainAcc)\n",
    "\n",
    "        # run validation\n",
    "        valLoss = sess.run(cost, feed_dict={x: valData, y: valLabelOneHot})\n",
    "        valAcc = sess.run(acc_op, feed_dict={x: valData, y: valLabelOneHot})\n",
    "\n",
    "        trainingPlot.Add(epoch_i, trainLoss, valLoss, trainAcc, valAcc)\n",
    "        plt.figure(1)\n",
    "        trainingPlot.Show()\n",
    "\n",
    "        # save snapshot\n",
    "        if resumeTraining and epoch_i % 10 == 0:\n",
    "            print \"training on image #%d\" % epoch_i\n",
    "            saver.save(sess, 'progress', global_step=epoch_i)\n",
    "\n",
    "        # show debugging image\n",
    "        if epoch_i % 500 == 0:\n",
    "            ShowDebuggingPlot()\n",
    "\n",
    "        # print(epoch_i, \"/\", n_epochs, loss)\n",
    "\n",
    "print(\"Training done. \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
